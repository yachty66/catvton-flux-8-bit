{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths\n",
    "image_path = \"./example/person/example1_model.jpg\"\n",
    "mask_path = \"./example/person/example1_model_mask.png\"\n",
    "garment_path = \"./example/garment/example1_clothes.jpg\"\n",
    "output_path_mask = \"./example/person/example1_model_mask.png\"\n",
    "output_path = \"example1.png\"\n",
    "resized_output_path = \"example1_resized.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=(576, 768)\n",
    "num_steps=50\n",
    "guidance_scale=30\n",
    "seed=4096\n",
    "pipe=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from diffusers import FluxTransformer2DModel\n",
    "\n",
    "# Login to Huggingface\n",
    "import os\n",
    "\n",
    "# Set up Hugging Face token\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"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\"\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f1c58a4de04f839496400b3965aa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550edec1ec634f36913d6a2c409b41c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df5efd03594d6d8b290ca4be5600fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae63d58f07d47609108bf6afe6814d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FluxTransformer2DModel(\n",
       "  (pos_embed): FluxPosEmbed()\n",
       "  (time_text_embed): CombinedTimestepGuidanceTextProjEmbeddings(\n",
       "    (time_proj): Timesteps()\n",
       "    (timestep_embedder): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=256, out_features=3072, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "    )\n",
       "    (guidance_embedder): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=256, out_features=3072, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "    )\n",
       "    (text_embedder): PixArtAlphaTextProjection(\n",
       "      (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act_1): SiLU()\n",
       "      (linear_2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (context_embedder): Linear(in_features=4096, out_features=3072, bias=True)\n",
       "  (x_embedder): Linear(in_features=384, out_features=3072, bias=True)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-18): 19 x FluxTransformerBlock(\n",
       "      (norm1): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
       "        (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (norm1_context): AdaLayerNormZero(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=3072, out_features=18432, bias=True)\n",
       "        (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (add_k_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (add_v_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (add_q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (to_add_out): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (norm_added_q): RMSNorm()\n",
       "        (norm_added_k): RMSNorm()\n",
       "      )\n",
       "      (norm2): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm2_context): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "      (ff_context): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=12288, out_features=3072, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (single_transformer_blocks): ModuleList(\n",
       "    (0-37): 38 x FluxSingleTransformerBlock(\n",
       "      (norm): AdaLayerNormZeroSingle(\n",
       "        (silu): SiLU()\n",
       "        (linear): Linear(in_features=3072, out_features=9216, bias=True)\n",
       "        (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "      )\n",
       "      (proj_mlp): Linear(in_features=3072, out_features=12288, bias=True)\n",
       "      (act_mlp): GELU(approximate='tanh')\n",
       "      (proj_out): Linear(in_features=15360, out_features=3072, bias=True)\n",
       "      (attn): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (to_k): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "        (to_v): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_out): AdaLayerNormContinuous(\n",
       "    (silu): SiLU()\n",
       "    (linear): Linear(in_features=3072, out_features=6144, bias=True)\n",
       "    (norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)\n",
       "  )\n",
       "  (proj_out): Linear(in_features=3072, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from diffusers.utils import load_image, check_min_version\n",
    "from diffusers import FluxPriorReduxPipeline, FluxFillPipeline\n",
    "from diffusers import FluxTransformer2DModel\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Build pipeline\n",
    "if pipe is None:\n",
    "    transformer = FluxTransformer2DModel.from_pretrained(\n",
    "        \"xiaozaa/catvton-flux-alpha\", \n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    pipe = FluxFillPipeline.from_pretrained(\n",
    "        \"black-forest-labs/FLUX.1-dev\",\n",
    "        transformer=transformer,\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "else:\n",
    "    pipe.to(\"cuda\")\n",
    "\n",
    "pipe.transformer.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/diffusers/image_processor.py:724: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,1.0]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fea6245b1241419a4322d0af6748fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total function time: 27.80 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # For RGB images\n",
    "])\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load and process images\n",
    "# print(\"image_path\", image_path)\n",
    "image = load_image(image_path).convert(\"RGB\").resize(size)\n",
    "mask = load_image(mask_path).convert(\"RGB\").resize(size)\n",
    "garment = load_image(garment_path).convert(\"RGB\").resize(size)\n",
    "\n",
    "# Transform images using the new preprocessing\n",
    "image_tensor = transform(image)\n",
    "mask_tensor = mask_transform(mask)[:1]  # Take only first channel\n",
    "garment_tensor = transform(garment)\n",
    "\n",
    "# Create concatenated images\n",
    "inpaint_image = torch.cat([garment_tensor, image_tensor], dim=2)  # Concatenate along width\n",
    "garment_mask = torch.zeros_like(mask_tensor)\n",
    "extended_mask = torch.cat([garment_mask, mask_tensor], dim=2)\n",
    "\n",
    "prompt = f\"The pair of images highlights a clothing and its styling on a model, high resolution, 4K, 8K; \" \\\n",
    "        f\"[IMAGE1] Detailed product shot of a clothing\" \\\n",
    "        f\"[IMAGE2] The same cloth is worn by a model in a lifestyle setting.\"\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
    "\n",
    "result = pipe(\n",
    "    height=size[1],\n",
    "    width=size[0] * 2,\n",
    "    image=inpaint_image,\n",
    "    mask_image=extended_mask,\n",
    "    num_inference_steps=num_steps,\n",
    "    generator=generator,\n",
    "    max_sequence_length=512,\n",
    "    guidance_scale=guidance_scale,\n",
    "    prompt=prompt,\n",
    ").images[0]\n",
    "\n",
    "# Split and save results\n",
    "width = size[0]\n",
    "garment_result = result.crop((0, 0, width, size[1]))\n",
    "tryon_result = result.crop((width, 0, width * 2, size[1]))\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total function time: {total_time:.2f} seconds\")\n",
    "\n",
    "tryon_result.save(output_path)\n",
    "# return garment_result, tryon_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alright next lets try some tensorrt stuff here"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
